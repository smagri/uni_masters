n


ial Intelligence





reality is often quite different. A real mobile robot rnay
have to navigate through rooms cluttered with furniture,
where walls may be hidden behind desks and bookshelves.
     A few researchers have implemented exploration sys-
tems using real robots. These robots have performed well,
but only within environments that satisfy certain restrictive
assumptions. For example, some systems are limited to
environments that can be explored using wall-following
[6], while others require that all walls intersect at right
angles and that these walls be unobstructed and visible to
the robot [9]. Some indoor environments meet these
requirements, but many do not.
     Our goal is to develop exploration strategies for the
complex environments typically found in real office build-
ings. Our approach is based on the detection of frontiers,
regions on the border between space known to be open and
unexplored space. In this paper, we describe how to detect
frontiers in occupancy grids and how to use frontiers to
guide exploration. Then we present results in which a real
mobile robot used frontier-based exploration to map envi-
ronments containing offices filled with furniture, hallways
lined with obstacles, narrow passages, and large open
spaces.

2.0 Frontier-Based Exploration

     The central question in exploration is: Given what you
know about the world, where should you move to gain as
much new information as possible? Initially, you know
nothing except what you can see from where you’re stand-
ing. You want to build a map that describes as much of the
world as possible, and you want to build this map as
quickly as possible.
     The central idea behind frontier-based exploration is:
To gain the most new information about the world, move to
the boundary between open space and uncharted territory.
     Frontiers are regions on the boundary between open
space and unexplored space. When a robot moves to a






   open areas surrounded by unknown territory. As a result,
  the robot could waste a great deal of time trying to reach
  non-existent fronticrs.
       Fortunately, we have found a way to substantially
  reduce the effect of specular reflections on evidence grids.
  The standard evidence grid formulation assumes that each
  sensor reading is independent of every other sensor read-
  ing. In reality, this is not the case-and we take advantage
  of this with a technique we call laser-limited sonar. We
    use a laser rangefinder in combination with the sonar sen-
  sors, and if the laser returns a range reading less than the
    sonar reading, we update the evidence grid as if the sonar
  had returned the range indicated by the laser, in addition to
  marking the cells actually returned by the laser as occu-
  pied.
       Why not just use the laser? Because the laser operates
  in a two-dimensional plane, while the sonar projects a
  three-dimensional cone. !So, any object that is above or
  below the laser plane will be invisible to the laser, but still
  detectable by the sonar. Another alternative would be to
  use a three-dimensional laser rangefinder, but at present,
  such devices are too large, too expensive, and too power-
  hungry to be commonly available on mobile robots.
       Laser-limited sonar isn’t perfect-it is still possible to
  get specular reflections from obstacles undetected by the
  laser-but in practice, we have found that it drastically
  reduces the number of uncorrected specular reflections
  from walls and other large obstacles, which tend to be the
  major sources of errors in evidence grids built using sonar.

  2.2 Frontier Detection

        After an evidence grid has been constructed, each cell
  in the grid is classified by comparing its occupancy proba-
  bility to the initial (prior) probability assigned to all cells.
  This algorithm is not particularly sensitive to the specific
  value of this prior probability. (A value of 0.5 was used in
  all of the experiments described in this paper.)
        Each cell is placed into one of three classes:

  * open: occupancy probability < prior probability
  * unknown: occupancy probability = prior probability
    0   occupied: occupancy probability > prior probability


       A process analogous to edge detection and region

  extraction in computer vision is used to find the bound-

  aries between open space and unknown space. Any open

  cell adjacent to an unknown cell is labeled a frontier edge

  cell. Adjacent edge cells are grouped into frontier regions.

  Any frontier region above a certain minimum size

  (roughly the size of the robot) is considered a frontier.








......               . .. ..



.. .. .. .. .. ....

. .   .  .  .  .     ...
. .. .. .. .. .. .. .. ..
                                       ...
                                       .. ..                                     ....
                                                                                  .. ...

. .. .. .. .. .. .. .........
.  .  .  .  .  .  .                      .. .                                       ..
                                                                                     <


                                       ... ...

. .. .. .. .. .. .. ..



.......                                                                        $3

  . . ..........
                                     .. ..                                    ... ...

         ..........
. . .. .. .. ..............






                                      . .

......
......

......
......
...
..
..                 ....
                   ......
                      .. .. ..

..



                        ...
.....                   ...
                        ...
                        ...

.......
.........

. . . . . . . . . ...
......                  ...
                        ...



......
....                   ...
                       .. .. .

....
.....
......

                        ...
                        .. .. ..


......
......                   ...
.......
.......                  ,. .. ..
                                     . ... ,.....                                            ....
                         1..




  .......







   .   .  . . .           .. .. ..
                           ....
                              ..
                                                                                            .    .




...                         ....
                              ..

 .. ......           ....





                                             ib)                                           (c>

ce grid, (b) frontier edge segments, (c) frontier regions

                                                        When the robot reaches its destination, that location is
                                                   added to the list of previously visited frontiers. The robot
                                                   performs a 360 degree sensor sweep using laser-limited
                                                   sonar and adds the new information to the evidence grid.
                                                   Then the robot detects frontiers present in the updated grid
                                                   and attempts to navigate to the nearest accessible, unvis-
                                                   ited frontier.
                                                        If the robot is unable to make progress toward its des-
                                                   tination, then after a certain amount of time, the robot will
                                                   determine that the destination in inaccessible, and its loca-
                                                   tion will be added to the list of inaccessible frontiers. The
                                                   robot will then conduct a sensor sweep, update the evi-
                                                   dence grid, and attempt to navigate to the closest remain-
                                                   ing accessible, unvisited frontier.

                                                   3.0 Experiments

                                                       Frontier-based exploration has been implemented on a
                                                   Nomad 200 mobile robot equipped with a laser
                                                   rangefinder, sixteen sonar sensors, and sixteen infrared
                                                   sensors. Laser-limited sonar is used to build evidence
                                                   grids, combining the data from the forward-facing sonar
                                                   with the data returned from the (forward-facing) laser
                                                   rangefinder. At extremely short ranges (less than 16




                                            148
ments. The first environment included a hallway and an
djacent office. This environment contained chairs, desks,
ables, bookcases, filing cabinets, a sofa, a water cooler,
nd boxes of varying size and shape.
    Figure 2 shows the results from a trial in which the
 robot started in the hallway, used frontier-based explora-
ion to detect and enter an open doorway, and then
xplored the adjacent office (23 feet x 20 feet) thoroughly.





                  . . ..   . ..   ....... ... . ...... .   ...   .... ..   . ..   . .




ion of an office




























      Figure 3: Evidence grid from frontier-based
          exploration of a large labloffice area

 4.0 Related Work

      Considerable research has been done in robot map-

 building, but most of this research has been conducted in
 simulation [4] or with robots that passively observe the
 world as they are moved by a human controller [2] [3].
 However, a few systems for autonomous exploration have
 been implemented on real robots.
      Mataric [6] has developed Toto, a robot that combines
 reactive exploration, using wall-following and obstacle-
 avoidance, with a simple topological path planner. The

   reactive nature of Toto’s exploration limits its ability to
 map environments where wall-following is insufficient to
 explore the complex structure of the world.

      We previously developed a reactivehopological explo-
 ration system [lo] for ELDEN. This system had the
 advantage of being able to adapt its topological map to
 changes encountered in the environment. However, it also

 suffered the limitations of a purely reactivc exploration

 strategy, in terms of the size and complexity of the envi-
 ronments that it could explore efficiently.
      Connell [ I ] has developed a simple exploration sys-
 tem to demonstrate his SSS architecture. This system was
 limited to mapping hallways where doors and corridors
 intersect at 90 degree angles.
      Lee [5] has implemented Kuipers Spatial Semantic
 Hierarchy [4] on a real robot. However, this approach also





 6.0 Acknowledgments

    This work is supported by the Office of Naval
Research. Alan Schultz and John Grefenstette provided
useful comments on this research.

     References

     Jonathan Connell and Sridhar Mahadevan, “Rapid
     task learning for real robots,” in Robot Learning,
     Jon Connell and Sridhar Mahadevan, Eds., Boston,
     MA: Kluwer Academic, pp. 105-139, 1993.
     Sean Engelson, Pussive Map Learning and Visual
     Place Recognition, Ph.D. Thesis, Department of
     Computer Science, Yale University, 1994.
     David Kortenkamp, Cognitive Maps for Mobile
     Robots: A Representation for Mapping and
     Navigation, Ph.D. ‘Thesis, Electrical Engineering
     and Computer Science Department, University of
     Michigan, 1993.
     Benjamin Kuipers and Yung-Tai Byun, “A robot
     exploration and mapping strategy based on a
     semantic hierarchy of spatial representations,”
     Journal of Robotics and Autonomous Systems, 8:47-
     63, 1991.
     Wan Yik Lee, Spatial Semantic Hierarchy f o r a
     Physical Robot, Ph.D. Thesis, Department of
     Computer Sciences, The University of Texas at
     Austin, 1996.
     Maja Mataric, “Integration of representation into
     goal-driven    behavior-based      robots,”   IEEE
     Transactions on Robotics and Automation,
     8(3):304-312, June 1992.
     Hans Moravec and Albert0 Elfes, “High resolution
     maps from wide angle sonar,” Proceedings of the
     IEEE International Conference on Robotics and
     Automation, St. Louis, MO, 1985, pp. 116-121.
     Alan Schultz, William Adams, and John
     Grefenstette, “Continuous localization using
     evidence grids,” NCARAI Technical Report AIC-
     96-007, Naval Research Laboratory, Washington,
     DC, 1997.
     Sebastian Thrun arid Arno Bucken, “Integrating
     grid-based and topological maps for mobile robot
     navigation,” Proceedings of the Thirteenth National
     Conference on Art@cial Intelligence (AAAI-96),
     Portland, OR, August 1996, pp. 944-950.
     Brian Yamauchi and Randall Beer, “Spatial learning
     for navigation in dynamic environments,” IEEE
     Transactions on Systems, Man, and Cybernetics -
     Part B: Cybernetics, Special Issue on Learning
     Autonomous Robots, 26(3):496-505, June 1996.






