
#include "ros/ros.h"
#include "std_msgs/String.h"
#include "gazebo_msgs/ModelStates.h"
#include "tf/transform_datatypes.h"

#include "sensor_msgs/LaserScan.h"
#include "nav_msgs/Odometry.h"

#include <image_transport/image_transport.h>
#include <opencv2/highgui/highgui.hpp>
#include <cv_bridge/cv_bridge.h>

#include "a3_help/RequestGoal.h"

#include <sstream>
#include <iostream>
#include <string>

#include <thread>
#include <chrono>
#include <deque>
#include <mutex>
#include <random>

namespace enc = sensor_msgs::image_encodings;

/**
 * This node shows some connections and publishing images
 */


class GazeboRetrieve{

  ros::NodeHandle nh_;
  image_transport::ImageTransport it_;
  image_transport::Publisher image_pub_;
  ros::Subscriber sub1_;
  ros::Subscriber sub2_;
  image_transport::Subscriber sub3_;
  cv_bridge::CvImagePtr cvPtr_;
  ros::ServiceServer service_;

  int count_;//! A counter to allow executing items on N iterations
  double resolution_;//! size of OgMap in pixels

  struct DataBuffer
  {
    std::deque<geometry_msgs::Pose> poseDeq;
    std::deque<ros::Time> timeStampDeq;
    std::mutex buffer_mutex_;
  };
  DataBuffer buffer;//! And now we have our container

  struct ImageDataBuffer
  {
    std::deque<cv::Mat> imageDeq;
    std::deque<ros::Time> timeStampDeq;
    std::mutex buffer_mutex_;
  };
  ImageDataBuffer imageBuffer;//! And now we have our container


public:
  GazeboRetrieve(ros::NodeHandle nh)
    : nh_(nh), it_(nh)
  {
    sub1_ = nh_.subscribe("odom", 1000, &GazeboRetrieve::odomCallback,this);
    sub2_ = nh_.subscribe("scan", 10, &GazeboRetrieve::laserCallback,this);
    image_transport::ImageTransport it(nh);
    sub3_ = it.subscribe("map_image/full", 1,
			 &GazeboRetrieve::imageCallback,this);

    //Publishing an image ... just to show how
    image_pub_ = it_.advertise("test/image", 1);

    //Allowing an incoming service
    service_ = nh_.advertiseService("request_goal",
				    &GazeboRetrieve::requestGoal,this);

    // Below  is  how to  get  parameters  from command  line,  on
    // command line they need to be _param:=value
    //
    ros::NodeHandle pn("~");
    double mapSize;
    double resolution;
    pn.param<double>("map_size", mapSize, 20.0);
    pn.param<double>("resolution", resolution_, 0.1);

    count_ =0;
    cv::namedWindow("view",CV_WINDOW_NORMAL);
    cv::startWindowThread();
    cv::waitKey(5);

  }

  ~GazeboRetrieve()
  {
    cv::destroyWindow("view");
  }


  bool requestGoal(a3_help::RequestGoal::Request  &req,
		   a3_help::RequestGoal::Response &res)
  {
    // Current goal in world/global coordinates on OgMap???
    //
    ROS_INFO("request: x=%ld, y=%ld", (long int)req.x, (long int)req.y);
    res.ack = true;
    ROS_INFO("sending back response: [%d]", res.ack);
    return true;
  }

  void odomCallback(const nav_msgs::OdometryConstPtr& msg)
  {
    //Let's get the pose out from odometry message
    // REMEBER: on command line you can view entier msg as
    //rosmsg show nav_msgs/Odometry
      
    // Each time ROS publishes a  new Odometry message, that is data
    // from the  robot's motion sensors,  determine the pose  of the
    // robot.  Pose  being the  position and  rotation of  the robot
    // relative to a coordinate system.
    //
    // Save the  poses and  the time  at which  they occured  into a
    // deque.
    //
    geometry_msgs::Pose pose=msg->pose.pose;
    buffer.buffer_mutex_.lock();
    buffer.poseDeq.push_back(pose);
    buffer.timeStampDeq.push_back(msg->header.stamp);
    buffer.buffer_mutex_.unlock();

  }



  void laserCallback(const sensor_msgs::LaserScanConstPtr& msg)
  {
    //! Below sample cicrulates through the scan and finds closest
    //! point to robot.

    // d is radius or distance to the obstacle from the current
    // robot position.  The closest point sits at the end of this
    // radial distance vector.
    //
    // theta is angle
    // x = d cos(theta)
    // y = d sin(theta)
      
    double closest_point=msg->range_max;
    double angle=0;
    double x,y;
    for (unsigned int idx=0 ; idx < msg->ranges.size() ; idx++){
      if(msg->ranges.at(idx)<closest_point){
	closest_point=msg->ranges.at(idx);
	angle=msg->angle_min+(idx*msg->angle_increment);
      }
    }
    ros::Time timeLaser = msg->header.stamp;
    x = closest_point * cos(angle);
    y = closest_point * sin(angle);
    std::cout << timeLaser
	      << " Laser: [d,theta,x,y]=["
	      << closest_point <<","<< angle << "," << x << "," << y << "]"
	      << std::endl;

    
    // Need to store these closest points in a deque for processing in
    // seperateThread() later???
    
  }

  
  void imageCallback(const sensor_msgs::ImageConstPtr& msg)
  {
    //! Below code pushes the image and time to a deque, to share
    //! across threads.

    // Each  time ROS  publishes  a new  ogMap as  an  image on  the
    // "map_image/full" topic(see  map_to_mage_node.cpp), which this
    // node subscribes to:
    //
    // Save the  images and the  time at  which they occured  into a
    // deque.
    //
      
    // Also,  map_to_image_node.cpp publishes  to "map_image/full"
    // topic  and   this  node,  sample.cpp,  subscribes   to  the
    // "map_image/full"  topic.  See  rosocv_node/rosocvTalker.cpp
    // and  rosocv_node/rosocvListner.cpp  for example  code  that
    // does this.	

    try
    {
      if (enc::isColor(msg->encoding))
	cvPtr_ = cv_bridge::toCvCopy(msg, enc::BGR8);
      else
	cvPtr_ = cv_bridge::toCvCopy(msg, enc::MONO8);
    }
    catch (cv_bridge::Exception& e)
    {
      ROS_ERROR("cv_bridge exception: %s", e.what());
      return;
    }

    imageBuffer.buffer_mutex_.lock();
    imageBuffer.imageDeq.push_back(cvPtr_->image);
    imageBuffer.timeStampDeq.push_back(msg->header.stamp);
    if(imageBuffer.imageDeq.size()>2){
      imageBuffer.imageDeq.pop_front();
      imageBuffer.timeStampDeq.pop_front();
    }
    imageBuffer.buffer_mutex_.unlock();

  }


//   // Need to put into laserCallback??????????????????????????????
//   void findClosestFrontier(int pixelX, int pixelY){

// /// Determines the frontier closest to the current robot location.
    
// // pixelX = current robot location x pixel
// // pixelY = current robot location y pixel
    
    
// // For TESTING location is origin of ogmap.
// // int x=100;
// // int y=100;
// //
// // int curCellVal = ((int) cvPtr_->image.at<uchar>(y,x));
// // fprintf(stderr, "curCellVal=%d\n", curCellVal);
    
// //if(curCellVal) == 255; // is free space
//     int x=pixelX; // local x,y are current robot pose in pixels
//     int y=pixelY;
    
// // Map of frontierNodes map<key,value> sorted by key=x, with
// // corresponding value=y.
//     map<int,int> frontierNode;
    

//     curCellValue = ((int) cvPtr_->image.at<uchar>(y,x+1));
//     if( (curCellVal != 255) || (curCellVal != 0) ){
// // Current cell is:
// //   !white=FS(FreeSpace) OR black=O(Obsticle)
// //   thus it's grey=UnKownSpace.
//       frontierNode.insert(y, x); // store this frontier node
//       return; // no need to check other cells around (x,y)
//     }
//     curCellValue = ((int) cvPtr_->image.at<uchar>(y,x-1));
//     if( (curCellVal != 255) || (curCellVal != 0) ){
//       frontierNode.insert(y, x);
//       return;
//     }

    
//     curCellVal = ((int) cvPtr_->image.at<uchar>(y+1,x));
//     if( (curCellVal != 255) || (curCellVal != 0) ){
//       frontierNode.insert(y, x);
//       return;
//     }
//     curCellVal = ((int) cvPtr_->image.at<uchar>(y-1,x));
//     if( (curCellVal != 255) || (curCellVal != 0) ){
//       frontierNode.insert(y, x);
//       return;
//     }

    
//     curCellVal = ((int) cvPtr_->image.at<uchar>(y+1,x-1));
//     if( (curCellVal != 255) || (curCellVal != 0) ){
//       frontierNode.insert(y, x);
//       return;
//     }
//     curCellVal = ((int) cvPtr_->image.at<uchar>(y-1,x-1));
//     if( (curCellVal != 255) || (curCellVal != 0) ){
//       frontierNode.insert(y, x);
//       return;
//     }

    
//     curCellVal = ((int) cvPtr_->image.at<uchar>(y+1,x+1));
//     if( (curCellVal != 255) || (curCellVal != 0) ){
//       frontierNode.insert(y, x);
//       return;
//     }
//     curCellVal = ((int) cvPtr_->image.at<uchar>(y-1,x+1));
//     if( (curCellVal != 255) || (curCellVal != 0) ){
//       frontierNode.insert(y, x);
//       return;
//     }
    
      
//   } // end: findClosestFrontier()

      


  
  void seperateThread() {
    /**
     * The below loop runs until ros is shutdown, to ensure this
     * thread does not remain a zombie thread.
     *
     * The loop locks the buffer, checks the size
     * And then pulls items: the pose and timer_t
     * You can think if these could have been combined into one ...
     */

    double yaw,x,y;
    /// The below gets the current Ros Time
    ros::Time timeOdom = ros::Time::now();;
    ros::Time timeImage = ros::Time::now();;
    cv::Mat image;


    while (ros::ok()) {
      int deqSz =-1;

      // Only  access pose  buffer and  imageBuffer contents  when the
      // deques in  them are  not empty,  that is  when we  have robot
      // poses and ogMap images available.
      //
      // Mutex locking  is required for  access to the deques  as this
      // thread shares  them with the  main thread(or other  theads if
      // they  exist).  The  odomCallback  and imageCallback  callback
      // functions  that access  these deque's  are called  within the
      // main thread.
      //
      // The  main thread  allows aquiring  of robot  related data  in
      // realtime.  This thread, seperateThread(), does the processing
      // of  this  data in  non-realtime.   Just  when the  system/cpu
      // schedular allows it.
      //
      buffer.buffer_mutex_.lock();
      deqSz = buffer.poseDeq.size();
      if (deqSz > 0) {
	geometry_msgs::Pose pose=buffer.poseDeq.front();
	yaw = tf::getYaw(pose.orientation);
	x = pose.position.x;
	y = pose.position.y;
	timeOdom = buffer.timeStampDeq.front();
	buffer.poseDeq.pop_front();
	buffer.timeStampDeq.pop_front();
      }
      buffer.buffer_mutex_.unlock();

      //! Lock image buffer, take one message from deque and unlock it
      imageBuffer.buffer_mutex_.lock();
      if(imageBuffer.imageDeq.size()>0){
	image = imageBuffer.imageDeq.front();
	timeImage = imageBuffer.timeStampDeq.front();
	imageBuffer.imageDeq.pop_front();
	imageBuffer.timeStampDeq.pop_front();
      }
      imageBuffer.buffer_mutex_.unlock();

      // Let's  do   something  with  the  current   pose  and  opencv
      // image/ogMap; on every 100 iterations.
      //
      if (count_>100){
	count_=0;
	
	if(!image.empty()){
	  cv::Mat rgbImage;
	  // Convert image from on color space to another.
	  cv::cvtColor(image, rgbImage, CV_GRAY2RGB);
	  
	  if (image.rows > 60 && image.cols > 60){
	    // Draw a red dot of tickness 7, at the centre of the
	    // ogMap image.
	    //

	    // Draw  a red  dot at  the  x/y metres  positon from  the
	    // centre  of  the  ogMap  image.   This  encompasses  the
	    // orientation of the robot as  theta is the angle between
	    // the radial line  at to the x/y position an  the x axis.
	    // That is x=r*cos(theta) and y=r*sin(theta)
	    //
	    cv::circle(rgbImage,
	     	       cv::Point((x/resolution_)+(image.rows/2),
	    			 (y/resolution_)+(image.cols/2)),
	     	       10, CV_RGB(255,0,0),-1);

	    // Draw a blue dot at the centre of the ogMap image.
	    cv::circle(rgbImage,
		       cv::Point((image.rows/2), (image.cols/2)),
		       7, CV_RGB(0,0,255),-1);

	    // NOTE: The ogMap moves with  the robot at its centre and
	    // that the  velocity Forward  and Back commands  move the
	    // robot  on the  radial  line of  it's orientation.   The
	    // robot is always at the  centre of the ogMap.  Of course
	    // a non-zero angular velocity  changes the orientation of
	    // the robot and thus the orientation of the ogMap image.
	    
	    // Draw a red line from centre of ogMap image to the right
	    // most edge of it. Also placing blue dots on both ends of
	    // the line.
	    //
	    // cv::circle(rgbImage,
	    // 	       cv::Point((image.rows/2), (image.cols/2)),
	    // 	       2, CV_RGB(0,0,255), -1);
	    // cv::circle(rgbImage,
	    // 	       cv::Point(image.rows, image.cols),
	    // 	       4, CV_RGB(0,0,255), -1);
	    // cv::line(rgbImage, 
	    // 	     cv::Point((image.rows/2), (image.cols/2)),
	    // 	     cv::Point(image.rows, image.cols),
	    // 	     CV_RGB(255,0,0), 1);
	    //
	    //cv::circle(rgbImage,
	    //       cv::Point((image.rows/2), (image.cols/2)),
	    //	       7, CV_RGB(0,0,255),-1);

	  }
	  
	  std::cout << timeOdom
		    << " Odom: [x,y,yaw]=["
		    << x << "," << y << "," << yaw << "]"
		    << std::endl;
	  
	  std::cout << " x/y m/resoln_ from ogMap centre: pos [x,y]=["
		    << (x/resolution_) << "," 
		    << (y/resolution_) << "] "
		    << std::endl 
		    << " Pixel coordinates on the ogMap for this pos [x,y]=[" 
		    << (x/resolution_)+(image.rows/2) << ","
		    << (y/resolution_)+(image.cols/2) << "]"
		    << std::endl;

	  
	  // Determine closest frontier to current robot location.
	  //
	  // findClosestFrontier(pixelX, pixelY);
	  //
	  // Move robot to closest Frontier.
	  //
	  // Use laserscan deque to do this???

		    
	  // Update GUI Window
	  //
	  //cv::imshow("view", image);
	  //
	  // Display coloured image.
	  cv::imshow("view", rgbImage);
	  cv::waitKey(5);

	  // Convert cvImage to ROS image message.  To use with cvPtr_
	  // in imageCallback()
	  //
	  //cv::imshow("view", cvPtr->image);
	  //cv::waitKey(3);
	  sensor_msgs::ImagePtr msg
	    = cv_bridge::CvImage(std_msgs::Header(),
				 "rgb8", rgbImage).toImageMsg();
	  image_pub_.publish(msg);
	  
	}
      }
      else{
	count_++;
	// This delay slows the loop down for the sake of readability
	std::this_thread::sleep_for (std::chrono::milliseconds(10));
      }

    } // end: while(ros::ok())
    
  } // end: seperateThread()

  
}; // end: GazeboRetrive class definition.



int main(int argc, char **argv)
{


  /**
   
   * The ros::init()  function needs to see  argc and argv so  that it
   * can  perform  any ROS  arguments  and  name remapping  that  were
   * provided at  the command  line.  For programmatic  remappings you
   * can  use a  different version  of init()  which takes  remappings
   * directly, but  for most  command-line programs, passing  argc and
   * argv is the  easiest way to do it.  The  third argument to init()
   * is the name of the node.
   *
   * You must call one of the versions of ros::init() before using any
   * other part of the ROS system.
   */
  ros::init(argc, argv, "gazebo_retrieve");

  /**
   * NodeHandle is the main access point to communications with the
   * ROS system.  The first NodeHandle constructed will fully
   * initialize this node, and the last NodeHandle destructed will
   * close down the node.
   */
  ros::NodeHandle nh;

  /**
   * Let's start seperate thread first, to do that we need to create
   * object and thereafter start the thread on the function desired
   */
  std::shared_ptr<GazeboRetrieve> gc(new GazeboRetrieve(nh));
  std::thread t(&GazeboRetrieve::seperateThread,gc);

  /**
   * ros::spin() will enter a loop, pumping callbacks.  With this
   * version, all callbacks will be called from within this thread
   * (the main one).  ros::spin() will exit when Ctrl-C is pressed, or
   * the node is shutdown by the master.
   */
  ros::spin();

  /**
   * Let's cleanup everything, shutdown ros and join the thread
   */
  ros::shutdown();
  t.join();

  return 0;
}

